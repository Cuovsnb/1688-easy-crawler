#!/usr/bin/env python3
"""
æµ‹è¯•æµç¨‹æ§åˆ¶åŠŸèƒ½
"""

import sys
import os

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.core.crawler import Alibaba1688Crawler
from src.core.config import CrawlerConfig

def test_process_control():
    """æµ‹è¯•æµç¨‹æ§åˆ¶åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æµç¨‹æ§åˆ¶åŠŸèƒ½")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    config = CrawlerConfig()
    crawler = Alibaba1688Crawler(
        base_url="https://www.1688.com",
        headless=False,
        config=config
    )
    
    try:
        # æµ‹è¯•æµç¨‹æ§åˆ¶æ–¹æ³•
        print("\nğŸ“‹ æµ‹è¯•æµç¨‹æ§åˆ¶æ–‡ä»¶æ“ä½œ...")
        
        # åˆå§‹åŒ–æµç¨‹æ–‡ä»¶
        crawler._init_process_file()
        
        # è¯»å–æµç¨‹çŠ¶æ€
        status = crawler._read_process_status()
        print(f"åˆå§‹çŠ¶æ€: {status}")
        
        # æ›´æ–°ä¸€ä¸ªæ­¥éª¤çŠ¶æ€
        crawler._update_process_status("æ‰“å¼€æµè§ˆå™¨åŠ è½½ä¸»é¡µ", 1)
        
        # å†æ¬¡è¯»å–çŠ¶æ€
        status = crawler._read_process_status()
        print(f"æ›´æ–°åçŠ¶æ€: {status}")
        
        # è·å–å½“å‰æ­¥éª¤
        current_step = crawler._get_current_step()
        print(f"å½“å‰æ­¥éª¤: {current_step}")
        
        print("âœ… æµç¨‹æ§åˆ¶åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        crawler.close()

if __name__ == "__main__":
    test_process_control()
