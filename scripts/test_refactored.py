#!/usr/bin/env python3
"""
é‡æ„åä»£ç çš„æµ‹è¯•è„šæœ¬

ç”¨äºéªŒè¯é‡æ„åçš„ä»£ç æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
"""

import sys
import os
import traceback

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def test_imports():
    """æµ‹è¯•æ‰€æœ‰æ¨¡å—çš„å¯¼å…¥"""
    print("ğŸ” æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    try:
        # æµ‹è¯•æ ¸å¿ƒæ¨¡å—
        from src.core.config import CrawlerConfig
        from src.core.crawler import Alibaba1688Crawler
        print("âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•é©±åŠ¨æ¨¡å—
        from src.drivers.webdriver_manager import WebDriverManager
        from src.drivers.browser_utils import BrowserUtils
        print("âœ… é©±åŠ¨æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•å¤„ç†å™¨æ¨¡å—
        from src.handlers.login_handler import LoginHandler
        from src.handlers.popup_handler import PopupHandler
        from src.handlers.popup_closer import PopupCloser
        from src.handlers.page_handler import PageHandler
        print("âœ… å¤„ç†å™¨æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•æå–å™¨æ¨¡å—
        from src.extractors.product_extractor import ProductExtractor
        from src.extractors.page_analyzer import PageAnalyzer
        print("âœ… æå–å™¨æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•ç­–ç•¥æ¨¡å—
        from src.strategies.search_strategy import SearchStrategy
        from src.strategies.url_builder import URLBuilder
        print("âœ… ç­–ç•¥æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•å·¥å…·æ¨¡å—
        from src.utils.cache_manager import CacheManager
        from src.utils.data_exporter import DataExporter
        from src.utils.helpers import setup_logging, get_random_delay, clean_text
        print("âœ… å·¥å…·æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_config():
    """æµ‹è¯•é…ç½®ç±»"""
    print("\nğŸ”§ æµ‹è¯•é…ç½®ç±»...")
    
    try:
        from src.core.config import CrawlerConfig
        
        config = CrawlerConfig()
        
        # æµ‹è¯•åŸºæœ¬é…ç½®
        assert config.DEFAULT_BASE_URL == "https://www.1688.com"
        assert config.GLOBAL_BASE_URL == "https://global.1688.com"
        print("âœ… åŸºç¡€URLé…ç½®æ­£ç¡®")
        
        # æµ‹è¯•æœç´¢URLç”Ÿæˆ
        search_url = config.get_search_url(config.DEFAULT_BASE_URL)
        assert search_url == "https://www.1688.com/s/offer_search.htm"
        print("âœ… æœç´¢URLç”Ÿæˆæ­£ç¡®")
        
        # æµ‹è¯•Chromeé€‰é¡¹
        options = config.get_all_chrome_options()
        assert len(options) > 0
        print("âœ… Chromeé€‰é¡¹é…ç½®æ­£ç¡®")
        
        # æµ‹è¯•ç›®å½•åˆ›å»º
        config.ensure_directories()
        print("âœ… ç›®å½•åˆ›å»ºåŠŸèƒ½æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®ç±»æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_url_builder():
    """æµ‹è¯•URLæ„é€ å™¨"""
    print("\nğŸ”— æµ‹è¯•URLæ„é€ å™¨...")
    
    try:
        from src.strategies.url_builder import URLBuilder
        from src.core.config import CrawlerConfig
        
        config = CrawlerConfig()
        url_builder = URLBuilder(config)
        
        # æµ‹è¯•æœç´¢URLæ„é€ 
        keyword = "æ‰‹æœº"
        urls = url_builder.build_search_urls(keyword)
        assert len(urls) > 0
        print(f"âœ… æˆåŠŸæ„é€  {len(urls)} ä¸ªæœç´¢URL")
        
        # æµ‹è¯•URLè§£æ
        test_url = urls[0]
        parsed = url_builder.parse_search_url(test_url)
        assert 'keyword' in parsed
        print("âœ… URLè§£æåŠŸèƒ½æ­£å¸¸")
        
        # æµ‹è¯•URLéªŒè¯
        is_valid = url_builder.validate_search_url(test_url)
        assert is_valid
        print("âœ… URLéªŒè¯åŠŸèƒ½æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ URLæ„é€ å™¨æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_data_exporter():
    """æµ‹è¯•æ•°æ®å¯¼å‡ºå™¨"""
    print("\nğŸ“Š æµ‹è¯•æ•°æ®å¯¼å‡ºå™¨...")
    
    try:
        from src.utils.data_exporter import DataExporter
        from src.core.config import CrawlerConfig
        
        config = CrawlerConfig()
        exporter = DataExporter(config)
        
        # æµ‹è¯•æ•°æ®
        test_products = [
            {
                'title': 'æµ‹è¯•å•†å“1',
                'price': 'ï¿¥100',
                'shop': 'æµ‹è¯•åº—é“º1',
                'sales': '10äººä»˜æ¬¾',
                'link': 'https://example.com/1',
                'image': 'https://example.com/img1.jpg'
            },
            {
                'title': 'æµ‹è¯•å•†å“2',
                'price': 'ï¿¥200',
                'shop': 'æµ‹è¯•åº—é“º2',
                'sales': '20äººä»˜æ¬¾',
                'link': 'https://example.com/2',
                'image': 'https://example.com/img2.jpg'
            }
        ]
        
        # æµ‹è¯•Excelå¯¼å‡º
        excel_file = exporter.save_to_excel(test_products, "æµ‹è¯•")
        if excel_file and os.path.exists(excel_file):
            print("âœ… Excelå¯¼å‡ºåŠŸèƒ½æ­£å¸¸")
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            try:
                os.remove(excel_file)
            except:
                pass
        else:
            print("âš ï¸ Excelå¯¼å‡ºå¯èƒ½æœ‰é—®é¢˜ï¼Œä½†ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½")
        
        # æµ‹è¯•JSONå¯¼å‡º
        json_file = exporter.save_to_json(test_products, "æµ‹è¯•")
        if json_file and os.path.exists(json_file):
            print("âœ… JSONå¯¼å‡ºåŠŸèƒ½æ­£å¸¸")
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            try:
                os.remove(json_file)
            except:
                pass
        else:
            print("âš ï¸ JSONå¯¼å‡ºå¯èƒ½æœ‰é—®é¢˜ï¼Œä½†ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¯¼å‡ºå™¨æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_helpers():
    """æµ‹è¯•å·¥å…·å‡½æ•°"""
    print("\nğŸ› ï¸ æµ‹è¯•å·¥å…·å‡½æ•°...")
    
    try:
        from src.utils.helpers import get_random_delay, clean_text, format_filename
        
        # æµ‹è¯•éšæœºå»¶è¿Ÿ
        delay = get_random_delay(1, 3)
        assert 1 <= delay <= 3
        print("âœ… éšæœºå»¶è¿ŸåŠŸèƒ½æ­£å¸¸")
        
        # æµ‹è¯•æ–‡æœ¬æ¸…ç†
        dirty_text = "  æµ‹è¯•æ–‡æœ¬\n\t  "
        clean = clean_text(dirty_text)
        assert clean == "æµ‹è¯•æ–‡æœ¬"
        print("âœ… æ–‡æœ¬æ¸…ç†åŠŸèƒ½æ­£å¸¸")
        
        # æµ‹è¯•æ–‡ä»¶åæ ¼å¼åŒ–
        filename = format_filename("æµ‹è¯•/æ–‡ä»¶å:*.txt")
        assert "/" not in filename and ":" not in filename and "*" not in filename
        print("âœ… æ–‡ä»¶åæ ¼å¼åŒ–åŠŸèƒ½æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ å·¥å…·å‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_crawler_creation():
    """æµ‹è¯•çˆ¬è™«åˆ›å»ºï¼ˆä¸å¯åŠ¨æµè§ˆå™¨ï¼‰"""
    print("\nğŸ•·ï¸ æµ‹è¯•çˆ¬è™«åˆ›å»º...")
    
    try:
        from src.core.crawler import Alibaba1688Crawler
        from src.core.config import CrawlerConfig
        
        # åªæµ‹è¯•é…ç½®å’Œæ¨¡å—åˆå§‹åŒ–ï¼Œä¸åˆ›å»ºWebDriver
        config = CrawlerConfig()
        
        # æµ‹è¯•é…ç½®æ˜¯å¦æ­£ç¡®
        assert hasattr(config, 'DEFAULT_BASE_URL')
        assert hasattr(config, 'PRODUCT_SELECTORS')
        assert hasattr(config, 'POPUP_SELECTORS')
        print("âœ… çˆ¬è™«é…ç½®æ­£ç¡®")
        
        # æµ‹è¯•ç±»å®šä¹‰
        assert hasattr(Alibaba1688Crawler, 'search_products')
        assert hasattr(Alibaba1688Crawler, 'search_products_strict_flow')
        assert hasattr(Alibaba1688Crawler, 'save_to_excel')
        print("âœ… çˆ¬è™«ç±»å®šä¹‰æ­£ç¡®")
        
        return True
        
    except Exception as e:
        print(f"âŒ çˆ¬è™«åˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹é‡æ„ä»£ç æµ‹è¯•")
    print("=" * 50)
    
    tests = [
        ("æ¨¡å—å¯¼å…¥", test_imports),
        ("é…ç½®ç±»", test_config),
        ("URLæ„é€ å™¨", test_url_builder),
        ("æ•°æ®å¯¼å‡ºå™¨", test_data_exporter),
        ("å·¥å…·å‡½æ•°", test_helpers),
        ("çˆ¬è™«åˆ›å»º", test_crawler_creation)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å‡ºé”™: {e}")
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é‡æ„ä»£ç åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³æ¨¡å—")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
