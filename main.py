#!/usr/bin/env python3
"""
1688å•†å“çˆ¬è™«ä¸»ç¨‹åº

é‡æž„åŽçš„1688çˆ¬è™«ç¨‹åºå…¥å£ï¼Œæä¾›ç”¨æˆ·å‹å¥½çš„äº¤äº’ç•Œé¢
"""

import sys
import os
import logging
from typing import Optional

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.core.crawler import Alibaba1688Crawler
from src.core.config import CrawlerConfig


def is_interactive() -> bool:
    """æ£€æŸ¥æ˜¯å¦åœ¨äº¤äº’å¼çŽ¯å¢ƒä¸­è¿è¡Œ"""
    try:
        return sys.stdin.isatty() and sys.stdout.isatty()
    except:
        return False


def get_user_input() -> tuple:
    """
    èŽ·å–ç”¨æˆ·è¾“å…¥
    :return: (keyword, pages, base_url, flow_choice)
    """
    try:
        if not is_interactive():
            # éžäº¤äº’å¼æ¨¡å¼ä½¿ç”¨é»˜è®¤å€¼
            print("ðŸ¤– éžäº¤äº’å¼æ¨¡å¼ï¼Œä½¿ç”¨é»˜è®¤å€¼...")
            return "æ‰‹æœº", 1, "https://www.1688.com", "1"

        print("=" * 60)
        print("ðŸš€ 1688å•†å“çˆ¬è™« - é‡æž„ç‰ˆ")
        print("=" * 60)

        # é€‰æ‹©æœç´¢æµç¨‹
        print("\nðŸ“‹ é€‰æ‹©æœç´¢æµç¨‹ï¼š")
        print("1. æ™ºèƒ½æµç¨‹ï¼ˆä¼˜å…ˆç¼“å­˜URLï¼Œè‡ªåŠ¨é™çº§ï¼ŒæŽ¨èï¼‰")
        print("2. ä¸¥æ ¼æµç¨‹ï¼ˆæŒ‰æŒ‡å®šæ­¥éª¤æ‰§è¡Œï¼šä¸»é¡µâ†’å¼¹çª—â†’æœç´¢â†’æ–°æ ‡ç­¾é¡µURLæž„é€ ï¼‰")
        print("3. æµç¨‹æŽ§åˆ¶ï¼ˆä¸¥æ ¼æŒ‰ç…§process.txtæ–‡ä»¶æ­¥éª¤æ‰§è¡Œï¼Œç”¨æˆ·ç¡®è®¤æ¯æ­¥ï¼‰")
        flow_choice = input("è¯·é€‰æ‹©æµç¨‹ (1-3, é»˜è®¤: 1): ").strip() or "1"

        # èŽ·å–æœç´¢å…³é”®è¯
        keyword = input("\nðŸ” è¯·è¾“å…¥è¦æœç´¢çš„å•†å“(é»˜è®¤: æ‰‹æœº): ").strip() or "æ‰‹æœº"

        # èŽ·å–çˆ¬å–é¡µæ•°
        pages_input = input("ðŸ“„ è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•°(é»˜è®¤: 1): ").strip()
        pages = int(pages_input) if pages_input.isdigit() and int(pages_input) > 0 else 1

        # é€‰æ‹©ç«™ç‚¹ç‰ˆæœ¬
        print("\nðŸŒ é€‰æ‹©ç«™ç‚¹ç‰ˆæœ¬ï¼š")
        print("1. å›½é™…ç«™ (global.1688.com)")
        print("2. ä¸­æ–‡ç«™ (1688.com)")
        site_choice = input("è¯·é€‰æ‹©ç«™ç‚¹ (1-2, é»˜è®¤: 2): ").strip() or "2"

        if site_choice == "1":
            base_url = "https://global.1688.com"
        else:
            base_url = "https://www.1688.com"

        return keyword, pages, base_url, flow_choice

    except (EOFError, KeyboardInterrupt):
        # å¦‚æžœè¾“å…¥è¢«ä¸­æ–­ï¼Œä½¿ç”¨é»˜è®¤å€¼
        print("\nðŸ¤– æ£€æµ‹åˆ°è¾“å…¥ä¸­æ–­ï¼Œä½¿ç”¨é»˜è®¤å€¼...")
        return "æ‰‹æœº", 1, "https://www.1688.com", "1"
    except Exception as e:
        print(f"âŒ èŽ·å–ç”¨æˆ·è¾“å…¥æ—¶å‡ºé”™: {e}")
        return "æ‰‹æœº", 1, "https://www.1688.com", "1"


def print_results_summary(products: list, keyword: str):
    """æ‰“å°ç»“æžœæ‘˜è¦"""
    if not products:
        print("\nâŒ æœªèŽ·å–åˆ°å•†å“æ•°æ®")
        print("å¯èƒ½åŽŸå› ï¼š")
        print("  1. æœç´¢æ¡ä»¶æ— ç»“æžœ")
        print("  2. éœ€è¦ç™»å½•æˆ–éªŒè¯ç ")
        print("  3. ç½‘ç«™ç»“æž„å·²æ›´æ–°")
        print("  4. ç½‘ç»œè¿žæŽ¥é—®é¢˜")
        return

    print(f"\nâœ… æŠ“å–å®Œæˆï¼")
    print(f"ðŸ“Š æœç´¢å…³é”®è¯: {keyword}")
    print(f"ðŸ“¦ èŽ·å–å•†å“æ•°é‡: {len(products)}")

    # æ˜¾ç¤ºå‰å‡ ä¸ªå•†å“çš„ç®€è¦ä¿¡æ¯
    print(f"\nðŸ“‹ å•†å“é¢„è§ˆï¼ˆå‰3ä¸ªï¼‰ï¼š")
    for i, product in enumerate(products[:3], 1):
        title = product.get('title', 'æœªçŸ¥å•†å“')[:50]
        price = product.get('price', 'ä»·æ ¼é¢è®®')
        shop = product.get('shop', 'æœªçŸ¥åº—é“º')[:20]
        print(f"  {i}. {title}... | {price} | {shop}")

    if len(products) > 3:
        print(f"  ... è¿˜æœ‰ {len(products) - 3} ä¸ªå•†å“")


def main():
    """ä¸»å‡½æ•°"""
    crawler = None

    try:
        # èŽ·å–ç”¨æˆ·è¾“å…¥
        keyword, pages, base_url, flow_choice = get_user_input()

        print(f"\nðŸ”§ é…ç½®ä¿¡æ¯ï¼š")
        print(f"   æœç´¢å…³é”®è¯: {keyword}")
        print(f"   çˆ¬å–é¡µæ•°: {pages}")
        print(f"   ä½¿ç”¨ç«™ç‚¹: {base_url}")
        flow_names = {'1': 'æ™ºèƒ½æµç¨‹', '2': 'ä¸¥æ ¼æµç¨‹', '3': 'æµç¨‹æŽ§åˆ¶'}
        print(f"   æœç´¢æµç¨‹: {flow_names.get(flow_choice, 'æ™ºèƒ½æµç¨‹')}")

        # åˆ›å»ºé…ç½®å¯¹è±¡
        config = CrawlerConfig()

        # åˆ›å»ºçˆ¬è™«å®žä¾‹
        print(f"\nðŸš€ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        crawler = Alibaba1688Crawler(
            base_url=base_url,
            headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
            config=config
        )

        # æ‰§è¡Œæœç´¢
        print(f"\nðŸ” å¼€å§‹æœç´¢...")
        if flow_choice == "2":
            print("ðŸ“‹ ä½¿ç”¨ä¸¥æ ¼æµç¨‹æœç´¢...")
            print("âš ï¸  æ³¨æ„ï¼šæ­¤æµç¨‹ä¼šæœ‰å¤šä¸ªç”¨æˆ·äº¤äº’æ­¥éª¤ï¼Œè¯·æ ¹æ®æç¤ºæ“ä½œ")
            products = crawler.search_products_strict_flow(keyword, pages=pages)
        elif flow_choice == "3":
            print("ðŸ”„ ä½¿ç”¨æµç¨‹æŽ§åˆ¶æœç´¢...")
            print("âš ï¸  æ³¨æ„ï¼šæ­¤æµç¨‹ä¼šä¸¥æ ¼æŒ‰ç…§process.txtæ–‡ä»¶çš„æ­¥éª¤æ‰§è¡Œï¼Œæ¯æ­¥éƒ½éœ€è¦ç”¨æˆ·ç¡®è®¤")
            products = crawler.search_products_with_process_control(keyword, pages=pages)
        else:
            print("ðŸ§  ä½¿ç”¨æ™ºèƒ½æµç¨‹æœç´¢...")
            products = crawler.search_products(keyword, pages=pages)

        # æ‰“å°ç»“æžœæ‘˜è¦
        print_results_summary(products, keyword)

        # ä¿å­˜æ•°æ®
        if products:
            print(f"\nðŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®...")

            # ä¿å­˜åˆ°Excel
            excel_filename = crawler.save_to_excel(products, keyword)
            if excel_filename:
                print(f"ðŸ“Š Excelæ–‡ä»¶: {excel_filename}")

            # ä¿å­˜åˆ°JSON
            json_filename = crawler.save_to_json(products, keyword)
            if json_filename:
                print(f"ðŸ“„ JSONæ–‡ä»¶: {json_filename}")

            if excel_filename or json_filename:
                print("âœ… æ•°æ®ä¿å­˜å®Œæˆï¼")
            else:
                print("âŒ æ•°æ®ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

        # æ˜¾ç¤ºçˆ¬è™«çŠ¶æ€
        if is_interactive():
            status = crawler.get_crawler_status()
            print(f"\nðŸ“ˆ çˆ¬è™«çŠ¶æ€:")
            print(f"   æ•°æ®æ¡æ•°: {status.get('data_count', 0)}")
            print(f"   å½“å‰é¡µé¢: {status.get('page_title', 'æœªçŸ¥')}")

    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        logging.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}", exc_info=True)
    finally:
        # æ¸…ç†èµ„æº
        if crawler:
            print(f"\nðŸ§¹ æ­£åœ¨æ¸…ç†èµ„æº...")
            crawler.close()

        print("ðŸ‘‹ ç¨‹åºç»“æŸ")


def run_batch_mode(keywords: list, base_url: str = "https://www.1688.com",
                   pages: int = 1, flow_choice: str = "1"):
    """
    æ‰¹é‡æ¨¡å¼è¿è¡Œ
    :param keywords: å…³é”®è¯åˆ—è¡¨
    :param base_url: åŸºç¡€URL
    :param pages: çˆ¬å–é¡µæ•°
    :param flow_choice: æµç¨‹é€‰æ‹©
    """
    print(f"ðŸ”„ æ‰¹é‡æ¨¡å¼ï¼šå¤„ç† {len(keywords)} ä¸ªå…³é”®è¯")

    config = CrawlerConfig()

    with Alibaba1688Crawler(base_url=base_url, headless=False, config=config) as crawler:
        for i, keyword in enumerate(keywords, 1):
            try:
                print(f"\nðŸ“ [{i}/{len(keywords)}] å¤„ç†å…³é”®è¯: {keyword}")

                if flow_choice == "2":
                    products = crawler.search_products_strict_flow(keyword, pages=pages)
                elif flow_choice == "3":
                    products = crawler.search_products_with_process_control(keyword, pages=pages)
                else:
                    products = crawler.search_products(keyword, pages=pages)

                if products:
                    # ä¿å­˜æ•°æ®
                    excel_filename = crawler.save_to_excel(products, keyword)
                    json_filename = crawler.save_to_json(products, keyword)

                    print(f"âœ… {keyword}: èŽ·å– {len(products)} ä¸ªå•†å“")
                    if excel_filename:
                        print(f"   ðŸ“Š Excel: {excel_filename}")
                    if json_filename:
                        print(f"   ðŸ“„ JSON: {json_filename}")
                else:
                    print(f"âŒ {keyword}: æœªèŽ·å–åˆ°å•†å“")

            except Exception as e:
                print(f"âŒ å¤„ç†å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {e}")
                logging.error(f"å¤„ç†å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {e}")
                continue

    print("ðŸŽ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")


def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    help_text = """
ðŸš€ 1688å•†å“çˆ¬è™« - é‡æž„ç‰ˆ

ç”¨æ³•:
    python main.py                    # äº¤äº’å¼æ¨¡å¼
    python main.py --help            # æ˜¾ç¤ºå¸®åŠ©
    python main.py --batch keywords.txt  # æ‰¹é‡æ¨¡å¼

å‚æ•°è¯´æ˜Ž:
    --help          æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
    --batch FILE    æ‰¹é‡æ¨¡å¼ï¼Œä»Žæ–‡ä»¶è¯»å–å…³é”®è¯åˆ—è¡¨
    --headless      æ— å¤´æ¨¡å¼è¿è¡Œ
    --site SITE     æŒ‡å®šç«™ç‚¹ (1688 æˆ– global)
    --pages N       çˆ¬å–é¡µæ•° (é»˜è®¤: 1)
    --flow FLOW     æœç´¢æµç¨‹ (1=æ™ºèƒ½, 2=ä¸¥æ ¼, 3=æµç¨‹æŽ§åˆ¶, é»˜è®¤: 1)

ç¤ºä¾‹:
    python main.py --batch keywords.txt --site 1688 --pages 2
    python main.py --headless --flow 2

æ‰¹é‡æ¨¡å¼æ–‡ä»¶æ ¼å¼:
    æ¯è¡Œä¸€ä¸ªå…³é”®è¯ï¼Œä¾‹å¦‚ï¼š
    æ‰‹æœº
    ç”µè„‘
    è€³æœº
    """
    print(help_text)


if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if "--help" in sys.argv or "-h" in sys.argv:
            show_help()
            sys.exit(0)
        elif "--batch" in sys.argv:
            try:
                batch_index = sys.argv.index("--batch")
                if batch_index + 1 < len(sys.argv):
                    keywords_file = sys.argv[batch_index + 1]

                    # è¯»å–å…³é”®è¯æ–‡ä»¶
                    with open(keywords_file, 'r', encoding='utf-8') as f:
                        keywords = [line.strip() for line in f if line.strip()]

                    # è§£æžå…¶ä»–å‚æ•°
                    base_url = "https://www.1688.com"
                    if "--site" in sys.argv:
                        site_index = sys.argv.index("--site")
                        if site_index + 1 < len(sys.argv):
                            site = sys.argv[site_index + 1]
                            if site.lower() == "global":
                                base_url = "https://global.1688.com"

                    pages = 1
                    if "--pages" in sys.argv:
                        pages_index = sys.argv.index("--pages")
                        if pages_index + 1 < len(sys.argv):
                            pages = int(sys.argv[pages_index + 1])

                    flow_choice = "1"
                    if "--flow" in sys.argv:
                        flow_index = sys.argv.index("--flow")
                        if flow_index + 1 < len(sys.argv):
                            flow_choice = sys.argv[flow_index + 1]

                    # è¿è¡Œæ‰¹é‡æ¨¡å¼
                    run_batch_mode(keywords, base_url, pages, flow_choice)
                else:
                    print("âŒ --batch å‚æ•°éœ€è¦æŒ‡å®šå…³é”®è¯æ–‡ä»¶")
                    sys.exit(1)
            except Exception as e:
                print(f"âŒ æ‰¹é‡æ¨¡å¼å‡ºé”™: {e}")
                sys.exit(1)
        else:
            print("âŒ æœªçŸ¥å‚æ•°ï¼Œä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©")
            sys.exit(1)
    else:
        # äº¤äº’å¼æ¨¡å¼
        main()
